{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77c25eb",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "## MODELLING THE MACHINE LEARNING ALGORITHMS FOR FRAUD CLASSIFICATION\n",
    "\n",
    "This Problem deals with the algorithm being able to correctly classify a Transaction as Fraudulent or not.\n",
    "It is a binary classification problem. <br />\n",
    "<br />\n",
    "Here I will fit four classification models :\n",
    "\n",
    "1) Logistic Regression\n",
    "\n",
    "2) Decision Tree\n",
    "\n",
    "3) Random Forest \n",
    "\n",
    "4) XGBOOST\n",
    "\n",
    "\n",
    "We will compare the performance of these algorithms and take a call as to which is suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26152f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries Required\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#set random seed for reproducibility\n",
    "np.random.seed(20)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6da0f87",
   "metadata": {},
   
   "source": [
    "#Load the Data\n",
    "\n",
    "df_preprocess = pd.read_pickle('data/pickles/df_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea54e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Percentage Statistics of Fraudulent vs Genuine Transactions [Non-Fraud: Class 1, Fraud : Class 0]\n",
    "\n",
    "fraud_trans = len(df_preprocess[df_preprocess.isFraud == 0])\n",
    "gen_trans = len(df_preprocess[df_preprocess.isFraud == 1])\n",
    "\n",
    "fraud_pctg = (fraud_trans)/(fraud_trans + gen_trans) * 100\n",
    "\n",
    "print(\"Number of Genuine transactions: \", gen_trans)\n",
    "print(\"Number of Fraud transactions: \", fraud_trans)\n",
    "print(\"Percentage of Fraud transactions: {:.4f}\".format(fraud_pctg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Class Imbalance \n",
    "\n",
    "sns.countplot(x ='isFraud', data = df_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us normalize fields for Logistic Regression \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def normalize(df): \n",
    "    # Define columns to normalize\n",
    "    fields = [\"creditLimit\", \"availableMoney\", \"transactionAmount\", \"transactionformatted\",\n",
    "              \"currentBalance\", \"timeSinceAccountOpening\", \"timeTillExp\", \"timeSinceAddressChange\"]\n",
    "    \n",
    "    for j in fields:\n",
    "        df[j] = preprocessing.normalize(df[[j]], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_normalized = normalize(df_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd87786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our Dependent and Independent Variables i.e X and Y\n",
    "\n",
    "Y = df_normalized[\"isFraud\"]\n",
    "X = df_normalized.drop([\"isFraud\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a699d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Undersampling to Address Class Imbalance in our Data \n",
    "\n",
    "sample = imblearn.under_sampling.RandomUnderSampler(random_state = 42)\n",
    "X, Y = sample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d11905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#Split Data into Training and Testing Sets\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "#Display Train and Test Set Shapes\n",
    "print(\"Training Set Shape : \" ,X_train.shape)\n",
    "print(\"Testing Set Shape : \" ,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d804e2",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Logistic Regression to our Data \n",
    "\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "Log_Regression = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "Log_Regression.fit(X_train, Y_train)\n",
    "Log_Reg_predictions = Log_Regression.predict(X_test)\n",
    "\n",
    "Log_Reg_predictions_prob = Log_Regression.predict_proba(X_test)\n",
    "Log_Reg_Roc = [Y_test, Log_Reg_predictions_prob ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I write Reusable function that calculates the imporatant metrics of classification algorithms \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score,roc_auc_score, ConfusionMatrixDisplay,roc_curve\n",
    "\n",
    "def display_classification_metrics(actuals, predictions):\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    prec = precision_score(actuals, predictions)\n",
    "    rec = recall_score(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions)\n",
    "    cm = confusion_matrix(actuals, predictions)\n",
    "    fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "    \n",
    "    # Print the classification metrics\n",
    "    print(\"Classification Metrics:\\n\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Accuracy:\\t {acc:.4f}\")\n",
    "    print(f\"Precision:\\t {prec:.4f}\")\n",
    "    print(f\"Recall:\\t\\t {rec:.4f}\")\n",
    "    print(f\"F1-score:\\t {f1:.4f}\")\n",
    "     \n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n",
    "    cmd.plot(cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that will calculate feature Importance score and plot it for 50 features\n",
    "\n",
    "def feature_importance(importance):\n",
    "    important_features = np.sort(importance)[:50]\n",
    "    num = len(important_features)\n",
    "    print(f\"Plotting Feature Importance Scores of {model} model for top {num} features:\")\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "   # summarize feature importance\n",
    "    for i,v in enumerate(importance[:num]):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "   # plot feature importance\n",
    "    plt.figure(figsize=(20,5))\n",
    "    pyplot.bar(np.arange(0,num), importance[:num], color = list('rgbkymc'))\n",
    "    plt.xticks(np.arange(0,num), np.array(X_train.columns[:num]), rotation = \"vertical\")\n",
    "    pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Logistic Regression Metrics\n",
    "\n",
    "print(\"Evaluation of Logistic Regression Model\")\n",
    "print(\"-------------------------------------------\")\n",
    "print()\n",
    "display_classification_metrics(Y_test, Log_Reg_predictions.round())\n",
    "print()\n",
    "print()\n",
    "print(\"ROC CURVE FOR LOGISTIC REGRESSION\")\n",
    "print(\"-------------------------------------------\")\n",
    "sklearn.metrics.plot_roc_curve(Log_Regression, X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance for Logistic Regression along with a plot\n",
    "\n",
    "model = \"Logistic Regression\"\n",
    "feature_importance(Log_Regression.coef_[0].argsort())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f580f",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73439354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fiting Decison Tree to our data \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "predictions_dt = decision_tree.predict(X_test)\n",
    "\n",
    "dt_predictions_prob = decision_tree.predict_proba(X_test)\n",
    "dt_Roc = [Y_test, dt_predictions_prob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cea250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Decision Tree Metrics\n",
    "\n",
    "print(\"Evaluation of Decision Tree Model\")\n",
    "print(\"-------------------------------------------\")\n",
    "display_classification_metrics(Y_test, predictions_dt.round())\n",
    "print()\n",
    "print()\n",
    "print(\"ROC CURVE FOR DECISION TREE\")\n",
    "print(\"-------------------------------------------\")\n",
    "sklearn.metrics.plot_roc_curve(decision_tree, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Imporatnce for Decision Tree\n",
    "model = \"Decision Tree\"\n",
    "feature_importance(decision_tree.feature_importances_.argsort())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Random Forest to our data \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators= 100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "predictions_rf = random_forest.predict(X_test)\n",
    "\n",
    "rf_predictions_prob = random_forest.predict_proba(X_test)\n",
    "rf_Roc = [Y_test, rf_predictions_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Random Forest Metrics\n",
    "\n",
    "print(\"Evaluation of Random Forest Model\")\n",
    "print(\"-------------------------------------------\")\n",
    "print()\n",
    "display_classification_metrics(Y_test, predictions_rf.round())\n",
    "print()\n",
    "print()\n",
    "print(\"ROC CURVE FOR RANDOM FOREST\")\n",
    "print(\"-------------------------------------------\")\n",
    "sklearn.metrics.plot_roc_curve(random_forest, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance for Random Forest\n",
    "model = \"Random Forest\"\n",
    "feature_importance(random_forest.feature_importances_.argsort())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b05402",
   "metadata": {},
   "source": [
    "##  eXtreme Gradient Boosting - XGBOOST \n",
    "\n",
    "I am using XGBOOST primarily because it has built-in regularization techniques,which help to prevent overfitting and improve the generalization performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit XGBOOST to data \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "XGB_Model = XGBClassifier(random_state=42)\n",
    "\n",
    "XGB_Model.fit(X_train, Y_train)\n",
    "predictions_XGB = XGB_Model.predict(X_test)\n",
    "\n",
    "XGB_pred_prob = XGB_Model.predict_proba(X_test)[:,1]\n",
    "xgb_roc = [Y_test, XGB_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display XGBOOST Metrics\n",
    "\n",
    "print(\"Evaluation of XGBOOST Model\")\n",
    "print(\"-------------------------------------------\")\n",
    "display_classification_metrics(Y_test, predictions_XGB.round())\n",
    "print()\n",
    "print()\n",
    "print(\"ROC CURVE FOR XGBOOST\")\n",
    "print(\"-------------------------------------------\")\n",
    "sklearn.metrics.plot_roc_curve(XGB_Model, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy is {Log_Regression.score(X_train, Y_train)}\")\n",
    "print(f\"Training accuracy is {decision_tree.score(X_train, Y_train)}\")\n",
    "print(f\"Training accuracy is {random_forest.score(X_train, Y_train)}\")\n",
    "print(f\"Testing accuracy is {XGB_Model.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48580c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the ROC Curves of all models together \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.lineplot(x=[0, 0.5, 1], y=[0, 0.5, 1], linestyle=\"dashed\")\n",
    "\n",
    "#Draw the Plots\n",
    "plot_roc_curve(XGB_Model, X_test, Y_test, ax=ax, label='XGB_Model')\n",
    "plot_roc_curve(random_forest, X_test, Y_test, ax=ax, label='Random Forest')\n",
    "plot_roc_curve(decision_tree, X_test, Y_test, ax=ax, label='Decision Tree')\n",
    "plot_roc_curve(Log_Regression, X_test, Y_test, ax=ax, label='Logistic Regression')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d229b3",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Models\n",
    "\n",
    "\n",
    "\n",
    "| Model  |  Training Accuracy | Testing Accuracy | Precision | Recall | F-1 Score | AUC |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| Logistic Regression | 0.6799 |  0.6721 | 0.6616  | 0.6978 | 0.6792 | 0.73 |\n",
    "| Decision Tree Classifier | 1.0 |  0.6372 | 0.6312 | 0.6508 | 0.6409 | 0.64 |\n",
    "| Random Forests | 1.0  | 0.7203 | 0.7175  | 0.7218 | 0.7197 | 0.80 |\n",
    "| XGBoost | 0.8268 |  0.7243 | 0.7217  |  0.7256 | 0.7236 | 0.80 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdaf8a",
   "metadata": {},
   "source": [
    "## RESULTS & CONCLUSIONS\n",
    "\n",
    "1) XGBOOST performs the best on this data \n",
    "\n",
    "2) None of the models can be used to make reliable predictions on this data.\n",
    "\n",
    "3) The Class Imabalance heavily affects the results we obtained, moreover undersampling removes a large amount of data.\n",
    "\n",
    "4) I would be interested to see how our models will perform with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0dc23",
   "metadata": {},
   "source": [
    "## FUTURE SCOPE \n",
    "\n",
    "1) Addressing Class Imbalance in a much more concrete manner, explore Synthetic Minority Over Sampling (SMOTE)\n",
    "\n",
    "2) We should consider some feature selection algorithms and parameter optimizations. This would be interesting to explore.\n",
    "\n",
    "3) Transaction Time is a very interesting variable that must be studied more carefully. Checking for trends or seasonality might lead to some interesting insights. Fraud prevelance around Public Holidays, Christmas, Easter etc can be studied.\n",
    "\n",
    "4) Explore some Ensemble Models to classify Fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bef98",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1) https://machinelearningmastery.com/ - Machine Learning Techniques and Code Support \n",
    "\n",
    "2) https://www.scirp.org/html/12-1501916_94450.htm - Research Paper Classifying Fraud in Automobile Insurance Claims \n",
    "\n",
    "3) https://marthawhite.github.io/mlcourse/notes.pdf - Machine Learning Handbook by Predrag Radivojac and Martha White"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
